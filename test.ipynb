{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def transform_mask(raw_mask):\n",
    "    c_true_mask = raw_mask.copy()\n",
    "    c_true_mask[0] = 0\n",
    "    c_true_mask[c_true_mask.sum()] = 0\n",
    "    c_true_mask = c_true_mask.astype(bool)\n",
    "    \n",
    "    return c_true_mask\n",
    "\n",
    "\n",
    "class HFLMScorer():\n",
    "    def __init__(self, model_name, device='cpu'):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def score_batch(self, batch):\n",
    "        input = self.tokenizer.batch_encode_plus([self.tokenizer.eos_token + s + self.tokenizer.eos_token for s in batch], padding=True, return_tensors='pt')\n",
    "        ids_np = input['input_ids'].detach().numpy()\n",
    "        ids = input['input_ids'].to(self.model.device)\n",
    "        mask = input['attention_mask'].numpy()\n",
    "        with torch.no_grad():\n",
    "            r = self.model(ids)[0]\n",
    "            r = torch.nn.LogSoftmax(dim=-1)(r).cpu().detach().numpy()\n",
    "\n",
    "        scores = []\n",
    "        for ci in range(r.shape[0]):\n",
    "            c_true_mask = transform_mask(mask[ci])\n",
    "            score = r[ci, range(c_true_mask.sum()), ids_np[ci][c_true_mask]].sum()\n",
    "            scores.append(score)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def score_sentences(self, sentences, split_size=32):\n",
    "        batch_count = len(sentences) // split_size + int(len(sentences) % split_size != 0)\n",
    "        scores = []\n",
    "        for i in tqdm(range(batch_count)):\n",
    "            scores += self.score_batch(sentences[i * split_size: (i + 1) * split_size])\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypernymysuite.evaluation import all_evaluations\n",
    "from hypernymysuite.base import HypernymySuiteModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GPTHypernymySuiteModel(HypernymySuiteModel):\n",
    "    def __init__(self, model, patterns, eval_data_dir):\n",
    "        #super(GPTHypernymySuiteModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.patterns = patterns\n",
    "        self.vocab['<OOV>'] = 1\n",
    "        for file_name in os.listdir(eval_data_dir):\n",
    "            file_path = os.path.join(eval_data_dir, file_name)\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "            for w in df['word1']:\n",
    "                self.vocab[w] = 1\n",
    "            for w in df['word2']:\n",
    "                self.vocab[w] = 1\n",
    "\n",
    "    def predict(self, hypo, hyper):\n",
    "        all_res = []\n",
    "        for pattern in self.patterns:\n",
    "            res = self.model.score_sentences([self.generate_sentence(pattern, hypo, hyper)])\n",
    "            all_res.append(res[0])\n",
    "        return np.mean(all_res)\n",
    "\n",
    "    def predict_many(self, hypos, hypers):\n",
    "        all_res = []\n",
    "        for pattern in self.patterns:\n",
    "            sentences = []\n",
    "            for x, y in zip(hypos, hypers):\n",
    "                sentences.append(self.generate_sentence(pattern, x, y))\n",
    "            res = np.array(self.model.score_sentences(sentences))\n",
    "            all_res.append(res)\n",
    "        #print(all_res)\n",
    "        return np.mean(all_res, axis=0)\n",
    "\n",
    "    def generate_sentence(self, pattern, hypo, hyper):\n",
    "        return pattern.replace('<hypo>', hypo).replace('<hyper>', hyper)\n",
    "    \n",
    "def print_res_table(res, return_mean=False):\n",
    "    metrics = []\n",
    "    metrics.append(res['siege_bless']['other']['ap_test_inv'])\n",
    "    metrics.append(res['siege_eval']['other']['ap_test_inv'])\n",
    "    metrics.append(res['siege_leds']['other']['ap_test_inv'])\n",
    "    metrics.append(res['siege_shwartz']['other']['ap_test_inv'])\n",
    "    metrics.append(res['siege_weeds']['other']['ap_test_inv'])\n",
    "\n",
    "    metrics.append(res['dir_dbless']['acc_test_inv'])\n",
    "    metrics.append(res['dir_wbless']['acc_test_inv'])\n",
    "    metrics.append(res['dir_bibless']['acc_test_inv'])\n",
    "\n",
    "    metrics.append(res['cor_hyperlex']['rho_test_inv'])\n",
    "    mean = np.mean(metrics)\n",
    "    metrics.append(mean)\n",
    "    metrics = [f'{val:.2f}'.replace('.', ',') for val in metrics]\n",
    "    if return_mean:\n",
    "        return ' '.join(metrics), mean\n",
    "    return ' '.join(metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'gen': \"<hyper> is more general than <hypo>\",\n",
    "    'spec': \"<hypo> is more specific than <hyper>\",\n",
    "    'type': \"<hypo> is a type of <hyper>\",\n",
    "    'hyper1': \"<hypo> which is a (example|class|kind|. . . ) of <hyper>\",\n",
    "    'hyper2': \"<hypo> which is a example of <hyper>\",\n",
    "    'hyper3': \"<hypo> which is a class of <hyper>\",\n",
    "    'hyper4': \"<hypo> which is a kind of <hyper>\",\n",
    "    'hyper5': \"<hypo> which is a type of <hyper>\",\n",
    "    'hyper6': \"<hypo> (and|or) (any|some) other <hyper>\",\n",
    "    'hyper7': \"<hypo> and any other <hyper>\",\n",
    "    'hyper8': \"<hypo> and some other <hyper>\",\n",
    "    'hyper9': \"<hypo> or any other <hyper>\",\n",
    "    'hyper10': \"<hypo> or some other <hyper>\",\n",
    "    'hyper11': \"<hypo> which is called <hyper>\",\n",
    "    'hyper12': \"<hypo> a special case of <hyper>\",\n",
    "    'hyper13': \"<hypo> is an <hyper> that\",\n",
    "    'hyper14': \"(Unlike|like) (most|all|any|other) <hyper>, <hypo>\",\n",
    "    'hyper15': \"unlike most <hyper>, <hypo>\",\n",
    "    'hyper16': \"unlike all <hyper>, <hypo>\",\n",
    "    'hyper17': \"unlike any <hyper>, <hypo>\",\n",
    "    'hyper18': \"unlike other <hyper>, <hypo>\",\n",
    "    'hyper19': \"like most <hyper>, <hypo>\",\n",
    "    'hyper20': \"like all <hyper>, <hypo>\",\n",
    "    'hyper21': \"like any <hyper>, <hypo>\",\n",
    "    'hyper22': \"like other <hyper>, <hypo>\",\n",
    "    'hyper23': \"<hyper> including <hypo>\",\n",
    "    'hyper24': \"such <hyper> as <hypo>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "patterns = ['<hypo> or some other <hyper>', '<hypo> or any other <hyper>']\n",
    "scorer = HFLMScorer('gpt2', 'cuda')\n",
    "hs_model = GPTHypernymySuiteModel(scorer, patterns, 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 124.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-29.846237, -29.836548, -30.710709], dtype=float32), array([-26.224533, -28.336967, -26.962746], dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-28.035385, -29.086758, -28.836727], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_model.predict_many(['cat', 'cat', 'cat'], ['animal', 'thing', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28.035384999999998"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5 * (-26.224533 + -29.846237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:01<00:00, 48.44it/s]\n",
      "100%|██████████| 53/53 [00:01<00:00, 50.34it/s]\n",
      "100%|██████████| 53/53 [00:01<00:00, 50.36it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 49.43it/s]\n",
      "100%|██████████| 42/42 [00:00<00:00, 49.05it/s]\n",
      "100%|██████████| 68/68 [00:01<00:00, 52.36it/s]\n",
      "100%|██████████| 452/452 [00:09<00:00, 47.56it/s]\n",
      "100%|██████████| 87/87 [00:01<00:00, 48.62it/s]\n",
      "100%|██████████| 421/421 [00:07<00:00, 59.32it/s]\n",
      "100%|██████████| 53/53 [00:01<00:00, 49.65it/s]\n",
      "100%|██████████| 1644/1644 [00:41<00:00, 39.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,38 0,35 0,81 0,47 0,83 0,91 0,72 0,64 0,35'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = all_evaluations(hs_model)\n",
    "print_res_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "pattern = '<hypo> or some other <hyper>'\n",
    "scorer = HFLMScorer('gpt2-medium', 'cuda')\n",
    "hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:02<00:00, 25.33it/s]\n",
      "100%|██████████| 53/53 [00:02<00:00, 25.79it/s]\n",
      "100%|██████████| 53/53 [00:02<00:00, 25.66it/s]\n",
      "100%|██████████| 42/42 [00:01<00:00, 25.28it/s]\n",
      "100%|██████████| 42/42 [00:01<00:00, 25.06it/s]\n",
      "100%|██████████| 68/68 [00:02<00:00, 26.37it/s]\n",
      "100%|██████████| 452/452 [00:18<00:00, 24.44it/s]\n",
      "100%|██████████| 87/87 [00:03<00:00, 25.01it/s]\n",
      "100%|██████████| 421/421 [00:13<00:00, 30.47it/s]\n",
      "100%|██████████| 53/53 [00:02<00:00, 25.55it/s]\n",
      "100%|██████████| 1644/1644 [01:18<00:00, 20.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,42 0,34 0,82 0,44 0,85 0,92 0,71 0,64 0,46'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = all_evaluations(hs_model)\n",
    "print_res_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "pattern = '<hypo> or some other <hyper>'\n",
    "scorer = HFLMScorer('gpt2-large', 'cuda')\n",
    "hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:03<00:00, 14.90it/s]\n",
      "100%|██████████| 53/53 [00:03<00:00, 15.07it/s]\n",
      "100%|██████████| 53/53 [00:03<00:00, 14.92it/s]\n",
      "100%|██████████| 42/42 [00:02<00:00, 14.68it/s]\n",
      "100%|██████████| 42/42 [00:02<00:00, 14.52it/s]\n",
      "100%|██████████| 68/68 [00:04<00:00, 15.21it/s]\n",
      "100%|██████████| 452/452 [00:32<00:00, 14.05it/s]\n",
      "100%|██████████| 87/87 [00:05<00:00, 14.53it/s]\n",
      "100%|██████████| 421/421 [00:24<00:00, 16.99it/s]\n",
      "100%|██████████| 53/53 [00:03<00:00, 14.95it/s]\n",
      "100%|██████████| 1644/1644 [02:23<00:00, 11.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,43 0,36 0,85 0,45 0,86 0,95 0,74 0,67 0,46'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = all_evaluations(hs_model)\n",
    "print_res_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '<hypo> or some other <hyper>'\n",
    "scorer = HFLMScorer('gpt2-xl', 'cuda')\n",
    "hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:06<00:00,  8.48it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.63it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.50it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  8.28it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  8.18it/s]\n",
      "100%|██████████| 68/68 [00:07<00:00,  8.75it/s]\n",
      "100%|██████████| 452/452 [00:57<00:00,  7.83it/s]\n",
      "100%|██████████| 87/87 [00:10<00:00,  8.06it/s]\n",
      "100%|██████████| 421/421 [00:43<00:00,  9.73it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.32it/s]\n",
      "100%|██████████| 1644/1644 [04:19<00:00,  6.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0,46 0,34 0,87 0,46 0,86 0,94 0,72 0,67 0,54'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = all_evaluations(hs_model)\n",
    "print_res_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dir_wbless': {'acc_val_inv': 0.8098712173183651,\n",
       "  'acc_test_inv': 0.7239376844430171},\n",
       " 'dir_bibless': {'acc_val_inv': 0.7572009182959314,\n",
       "  'acc_test_inv': 0.6664807387807571},\n",
       " 'dir_dbless': {'acc_val': 0.9285714285714286,\n",
       "  'acc_test': 0.9373433583959899,\n",
       "  'acc_all': 0.9364248317127898,\n",
       "  'acc_val_inv': 0.9285714285714286,\n",
       "  'acc_test_inv': 0.9373433583959899,\n",
       "  'acc_all_inv': 0.9364248317127898,\n",
       "  'num_val': 140,\n",
       "  'num_test': 1197,\n",
       "  'num_oov_all': 0,\n",
       "  'pct_oov_all': 0.0},\n",
       " 'cor_hyperlex': {'rho_train': 0.5586631768738135,\n",
       "  'rho_val': 0.5447425448058909,\n",
       "  'rho_test': 0.5604587457824581,\n",
       "  'rho_all': 0.5582632667721859,\n",
       "  'rho_train_inv': 0.5586631768738135,\n",
       "  'rho_val_inv': 0.5447425448058909,\n",
       "  'rho_test_inv': 0.5447425448058909,\n",
       "  'rho_all_inv': 0.5582632667721859,\n",
       "  'num_all': 2163,\n",
       "  'num_oov_all': 0,\n",
       "  'pct_oov_all': 0.0},\n",
       " 'siege_bless': {'other': {'ap_val': 0.44068169322138595,\n",
       "   'ap_test': 0.45638594417953743,\n",
       "   'ap100_val': 0.6696403942403542,\n",
       "   'ap100_test': 0.9388007091176314,\n",
       "   'ap_val_inv': 0.44068169322138595,\n",
       "   'ap_test_inv': 0.45638594417953743,\n",
       "   'ap100_val_inv': 0.6696403942403542,\n",
       "   'ap100_test_inv': 0.9388007091176314},\n",
       "  'pct_oov': 0.00749553018841975},\n",
       " 'siege_leds': {'other': {'ap_val': 0.8687294872007876,\n",
       "   'ap_test': 0.8697007498510032,\n",
       "   'ap100_val': 0.9570174200746513,\n",
       "   'ap100_test': 1.0,\n",
       "   'ap_val_inv': 0.8687294872007876,\n",
       "   'ap_test_inv': 0.8697007498510032,\n",
       "   'ap100_val_inv': 0.9570174200746513,\n",
       "   'ap100_test_inv': 1.0},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_eval': {'other': {'ap_val': 0.3166300634605544,\n",
       "   'ap_test': 0.3446342283210317,\n",
       "   'ap100_val': 0.549438084124,\n",
       "   'ap100_test': 0.5906867337457222,\n",
       "   'ap_val_inv': 0.3166300634605544,\n",
       "   'ap_test_inv': 0.3446342283210317,\n",
       "   'ap100_val_inv': 0.549438084124,\n",
       "   'ap100_test_inv': 0.5906867337457222},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_weeds': {'other': {'ap_val': 0.8138851956171183,\n",
       "   'ap_test': 0.8630517721881589,\n",
       "   'ap100_val': 0.8719767633206196,\n",
       "   'ap100_test': 1.0,\n",
       "   'ap_val_inv': 0.8138851956171183,\n",
       "   'ap_test_inv': 0.8630517721881589,\n",
       "   'ap100_val_inv': 0.8719767633206196,\n",
       "   'ap100_test_inv': 1.0},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_shwartz': {'other': {'ap_val': 0.4758931386476219,\n",
       "   'ap_test': 0.45633291820140787,\n",
       "   'ap100_val': 0.6040149194436693,\n",
       "   'ap100_test': 0.6376571763596333,\n",
       "   'ap_val_inv': 0.4758931386476219,\n",
       "   'ap_test_inv': 0.45633291820140787,\n",
       "   'ap100_val_inv': 0.6040149194436693,\n",
       "   'ap100_test_inv': 0.6376571763596333},\n",
       "  'pct_oov': 0.0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dir_wbless': {'acc_val_inv': 0.7994900147805672,\n",
       "  'acc_test_inv': 0.7087304270697137},\n",
       " 'dir_bibless': {'acc_val_inv': 0.7305384070006443,\n",
       "  'acc_test_inv': 0.6403372970854118},\n",
       " 'dir_dbless': {'acc_val': 0.9071428571428571,\n",
       "  'acc_test': 0.9239766081871345,\n",
       "  'acc_all': 0.9222139117427075,\n",
       "  'acc_val_inv': 0.9071428571428571,\n",
       "  'acc_test_inv': 0.9239766081871345,\n",
       "  'acc_all_inv': 0.9222139117427075,\n",
       "  'num_val': 140,\n",
       "  'num_test': 1197,\n",
       "  'num_oov_all': 0,\n",
       "  'pct_oov_all': 0.0},\n",
       " 'cor_hyperlex': {'rho_train': 0.4838805421075947,\n",
       "  'rho_val': 0.45648896447855586,\n",
       "  'rho_test': 0.4749770722254598,\n",
       "  'rho_all': 0.4807383302613861,\n",
       "  'rho_train_inv': 0.4838805421075947,\n",
       "  'rho_val_inv': 0.45648896447855586,\n",
       "  'rho_test_inv': 0.45648896447855586,\n",
       "  'rho_all_inv': 0.4807383302613861,\n",
       "  'num_all': 2163,\n",
       "  'num_oov_all': 0,\n",
       "  'pct_oov_all': 0.0},\n",
       " 'siege_bless': {'other': {'ap_val': 0.38770808700372295,\n",
       "   'ap_test': 0.4244664392510192,\n",
       "   'ap100_val': 0.5558835333674718,\n",
       "   'ap100_test': 0.9443982723030131,\n",
       "   'ap_val_inv': 0.38770808700372295,\n",
       "   'ap_test_inv': 0.4244664392510192,\n",
       "   'ap100_val_inv': 0.5558835333674718,\n",
       "   'ap100_test_inv': 0.9443982723030131},\n",
       "  'pct_oov': 0.00749553018841975},\n",
       " 'siege_leds': {'other': {'ap_val': 0.8367572621644055,\n",
       "   'ap_test': 0.8247427804856687,\n",
       "   'ap100_val': 0.9329898845346117,\n",
       "   'ap100_test': 0.9894857513340163,\n",
       "   'ap_val_inv': 0.8367572621644055,\n",
       "   'ap_test_inv': 0.8247427804856687,\n",
       "   'ap100_val_inv': 0.9329898845346117,\n",
       "   'ap100_test_inv': 0.9894857513340163},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_eval': {'other': {'ap_val': 0.32069069089033875,\n",
       "   'ap_test': 0.3409208917674984,\n",
       "   'ap100_val': 0.5067426068337351,\n",
       "   'ap100_test': 0.6277097708996559,\n",
       "   'ap_val_inv': 0.32069069089033875,\n",
       "   'ap_test_inv': 0.3409208917674984,\n",
       "   'ap100_val_inv': 0.5067426068337351,\n",
       "   'ap100_test_inv': 0.6277097708996559},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_weeds': {'other': {'ap_val': 0.8024352531474628,\n",
       "   'ap_test': 0.8463233444278176,\n",
       "   'ap100_val': 0.8385265172197942,\n",
       "   'ap100_test': 1.0,\n",
       "   'ap_val_inv': 0.8024352531474628,\n",
       "   'ap_test_inv': 0.8463233444278176,\n",
       "   'ap100_val_inv': 0.8385265172197942,\n",
       "   'ap100_test_inv': 1.0},\n",
       "  'pct_oov': 0.0},\n",
       " 'siege_shwartz': {'other': {'ap_val': 0.46516933501280044,\n",
       "   'ap_test': 0.44389321799868586,\n",
       "   'ap100_val': 0.6067224590200586,\n",
       "   'ap100_test': 0.48194575169466614,\n",
       "   'ap_val_inv': 0.46516933501280044,\n",
       "   'ap_test_inv': 0.44389321799868586,\n",
       "   'ap100_val_inv': 0.6067224590200586,\n",
       "   'ap100_test_inv': 0.48194575169466614},\n",
       "  'pct_oov': 0.0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'gen': \"<hyper> is more general than <hypo>\",\n",
    "    'spec': \"<hypo> is more specific than <hyper>\",\n",
    "    'type': \"<hypo> is a type of <hyper>\",\n",
    "    'hyper1': \"<hypo> which is a (example|class|kind|. . . ) of <hyper>\",\n",
    "    'hyper2': \"<hypo> which is a example of <hyper>\",\n",
    "    'hyper3': \"<hypo> which is a class of <hyper>\",\n",
    "    'hyper4': \"<hypo> which is a kind of <hyper>\",\n",
    "    'hyper5': \"<hypo> which is a type of <hyper>\",\n",
    "    'hyper6': \"<hypo> (and|or) (any|some) other <hyper>\",\n",
    "    'hyper7': \"<hypo> and any other <hyper>\",\n",
    "    'hyper8': \"<hypo> and some other <hyper>\",\n",
    "    'hyper9': \"<hypo> or any other <hyper>\",\n",
    "    'hyper10': \"<hypo> or some other <hyper>\",\n",
    "    'hyper11': \"<hypo> which is called <hyper>\",\n",
    "    'hyper12': \"<hypo> a special case of <hyper>\",\n",
    "    'hyper13': \"<hypo> is an <hyper> that\",\n",
    "    'hyper14': \"(Unlike|like) (most|all|any|other) <hyper>, <hypo>\",\n",
    "    'hyper15': \"unlike most <hyper>, <hypo>\",\n",
    "    'hyper16': \"unlike all <hyper>, <hypo>\",\n",
    "    'hyper17': \"unlike any <hyper>, <hypo>\",\n",
    "    'hyper18': \"unlike other <hyper>, <hypo>\",\n",
    "    'hyper19': \"like most <hyper>, <hypo>\",\n",
    "    'hyper20': \"like all <hyper>, <hypo>\",\n",
    "    'hyper21': \"like any <hyper>, <hypo>\",\n",
    "    'hyper22': \"like other <hyper>, <hypo>\",\n",
    "    'hyper23': \"<hyper> including <hypo>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2-xl'\n",
    "scorer = HFLMScorer(model_name, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:07<00:00,  6.80it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.04it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.00it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.76it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.82it/s]\n",
      "100%|██████████| 68/68 [00:09<00:00,  7.24it/s]\n",
      "100%|██████████| 452/452 [01:08<00:00,  6.60it/s]\n",
      "100%|██████████| 87/87 [00:13<00:00,  6.53it/s]\n",
      "100%|██████████| 421/421 [00:53<00:00,  7.83it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.72it/s]\n",
      "100%|██████████| 1644/1644 [05:06<00:00,  5.36it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hyper> is more general than <hypo>\n",
      "0,10 0,25 0,64 0,34 0,50 0,31 0,49 0,25 0,11 0,33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:07<00:00,  6.68it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.78it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.79it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.63it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.56it/s]\n",
      "100%|██████████| 68/68 [00:09<00:00,  7.10it/s]\n",
      "100%|██████████| 452/452 [01:11<00:00,  6.36it/s]\n",
      "100%|██████████| 87/87 [00:13<00:00,  6.48it/s]\n",
      "100%|██████████| 421/421 [00:54<00:00,  7.75it/s]\n",
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:817: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.68it/s]\n",
      "100%|██████████| 1644/1644 [05:07<00:00,  5.35it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> is more specific than <hyper>\n",
      "0,11 0,23 0,66 0,35 0,54 0,59 0,52 0,37 0,22 0,40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:07<00:00,  6.68it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.78it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.79it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.63it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.55it/s]\n",
      "100%|██████████| 68/68 [00:09<00:00,  7.10it/s]\n",
      "100%|██████████| 452/452 [01:11<00:00,  6.36it/s]\n",
      "100%|██████████| 87/87 [00:13<00:00,  6.48it/s]\n",
      "100%|██████████| 421/421 [00:54<00:00,  7.75it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  6.68it/s]\n",
      "100%|██████████| 1644/1644 [05:07<00:00,  5.35it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> is a type of <hyper>\n",
      "0,38 0,34 0,87 0,44 0,81 0,91 0,69 0,63 0,53 0,62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:14<00:00,  3.55it/s]\n",
      "100%|██████████| 53/53 [00:14<00:00,  3.56it/s]\n",
      "100%|██████████| 53/53 [00:14<00:00,  3.55it/s]\n",
      "100%|██████████| 42/42 [00:12<00:00,  3.49it/s]\n",
      "100%|██████████| 42/42 [00:12<00:00,  3.48it/s]\n",
      "100%|██████████| 68/68 [00:19<00:00,  3.57it/s]\n",
      "100%|██████████| 452/452 [02:12<00:00,  3.42it/s]\n",
      "100%|██████████| 87/87 [00:25<00:00,  3.47it/s]\n",
      "100%|██████████| 421/421 [01:51<00:00,  3.76it/s]\n",
      "100%|██████████| 53/53 [00:14<00:00,  3.54it/s]\n",
      "100%|██████████| 1644/1644 [08:50<00:00,  3.10it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> which is a (example|class|kind|. . . ) of <hyper>\n",
      "0,19 0,28 0,66 0,40 0,67 0,63 0,60 0,46 0,38 0,47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.33it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.13it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.08it/s]\n",
      "100%|██████████| 68/68 [00:10<00:00,  6.44it/s]\n",
      "100%|██████████| 452/452 [01:16<00:00,  5.92it/s]\n",
      "100%|██████████| 87/87 [00:14<00:00,  6.09it/s]\n",
      "100%|██████████| 421/421 [00:59<00:00,  7.07it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 1644/1644 [05:27<00:00,  5.02it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> which is a example of <hyper>\n",
      "0,19 0,25 0,69 0,39 0,70 0,80 0,62 0,54 0,33 0,50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.32it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.12it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.08it/s]\n",
      "100%|██████████| 68/68 [00:10<00:00,  6.44it/s]\n",
      "100%|██████████| 452/452 [01:16<00:00,  5.91it/s]\n",
      "100%|██████████| 87/87 [00:14<00:00,  6.08it/s]\n",
      "100%|██████████| 421/421 [00:59<00:00,  7.06it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 1644/1644 [05:27<00:00,  5.02it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> which is a class of <hyper>\n",
      "0,34 0,28 0,80 0,42 0,79 0,84 0,68 0,60 0,51 0,59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.33it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.13it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.08it/s]\n",
      "100%|██████████| 68/68 [00:10<00:00,  6.44it/s]\n",
      "100%|██████████| 452/452 [01:16<00:00,  5.92it/s]\n",
      "100%|██████████| 87/87 [00:14<00:00,  6.08it/s]\n",
      "100%|██████████| 421/421 [00:59<00:00,  7.06it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 1644/1644 [05:27<00:00,  5.02it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> which is a kind of <hyper>\n",
      "0,25 0,28 0,75 0,41 0,69 0,75 0,60 0,50 0,47 0,52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:08<00:00,  6.27it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.32it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.12it/s]\n",
      "100%|██████████| 42/42 [00:06<00:00,  6.07it/s]\n",
      "100%|██████████| 68/68 [00:10<00:00,  6.44it/s]\n",
      "100%|██████████| 452/452 [01:16<00:00,  5.91it/s]\n",
      "100%|██████████| 87/87 [00:14<00:00,  6.09it/s]\n",
      "100%|██████████| 421/421 [00:59<00:00,  7.05it/s]\n",
      "100%|██████████| 53/53 [00:08<00:00,  6.26it/s]\n",
      "100%|██████████| 1644/1644 [05:27<00:00,  5.02it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> which is a type of <hyper>\n",
      "0,28 0,29 0,77 0,42 0,71 0,78 0,61 0,52 0,47 0,54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:12<00:00,  4.28it/s]\n",
      "100%|██████████| 53/53 [00:12<00:00,  4.30it/s]\n",
      "100%|██████████| 53/53 [00:12<00:00,  4.27it/s]\n",
      "100%|██████████| 42/42 [00:10<00:00,  4.20it/s]\n",
      "100%|██████████| 42/42 [00:10<00:00,  4.18it/s]\n",
      "100%|██████████| 68/68 [00:15<00:00,  4.33it/s]\n",
      "100%|██████████| 452/452 [01:50<00:00,  4.10it/s]\n",
      "100%|██████████| 87/87 [00:20<00:00,  4.18it/s]\n",
      "100%|██████████| 421/421 [01:31<00:00,  4.62it/s]\n",
      "100%|██████████| 53/53 [00:12<00:00,  4.28it/s]\n",
      "100%|██████████| 1644/1644 [07:32<00:00,  3.63it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> (and|or) (any|some) other <hyper>\n",
      "0,35 0,36 0,75 0,47 0,79 0,84 0,67 0,60 0,42 0,58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:07<00:00,  7.48it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.54it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.44it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.28it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.20it/s]\n",
      "100%|██████████| 68/68 [00:08<00:00,  7.73it/s]\n",
      "100%|██████████| 452/452 [01:05<00:00,  6.95it/s]\n",
      "100%|██████████| 87/87 [00:12<00:00,  7.20it/s]\n",
      "100%|██████████| 421/421 [00:48<00:00,  8.73it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.47it/s]\n",
      "100%|██████████| 1644/1644 [04:48<00:00,  5.70it/s]\n",
      "  0%|          | 0/53 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> and any other <hyper>\n",
      "0,40 0,35 0,84 0,47 0,84 0,93 0,70 0,64 0,53 0,63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:07<00:00,  7.48it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.53it/s]\n",
      "100%|██████████| 53/53 [00:07<00:00,  7.44it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.28it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.20it/s]\n",
      "100%|██████████| 68/68 [00:08<00:00,  7.73it/s]\n",
      "  9%|▉         | 41/452 [00:05<00:57,  7.20it/s]"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "for prompt_name in PROMPTS:\n",
    "    pattern = PROMPTS[prompt_name]\n",
    "    hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n",
    "    res = all_evaluations(hs_model)\n",
    "    print(model_name, ' ', pattern)\n",
    "    print(print_res_table(res))\n",
    "\n",
    "    res['pattern'] = pattern\n",
    "    with codecs.open(f'output/{model_name}_{prompt_name}.json', 'w', 'utf-8') as file_descr:\n",
    "        json.dump(res, file_descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2   <hyper> is more general than <hypo>\n",
      "gpt2   <hypo> which is a (example|class|kind|. . . ) of <hyper>\n",
      "gpt2   <hypo> or some other <hyper>\n",
      "gpt2   <hypo> which is called <hyper>\n",
      "gpt2   <hypo> a special case of <hyper>\n",
      "gpt2   <hypo> is an <hyper> that\n",
      "gpt2   (Unlike|like) (most|all|any|other) <hyper>, <hypo>\n",
      "gpt2   unlike most <hyper>, <hypo>\n",
      "gpt2   unlike all <hyper>, <hypo>\n",
      "gpt2   unlike any <hyper>, <hypo>\n",
      "gpt2   unlike other <hyper>, <hypo>\n",
      "gpt2   like most <hyper>, <hypo>\n",
      "gpt2   <hypo> which is a example of <hyper>\n",
      "gpt2   like all <hyper>, <hypo>\n",
      "gpt2   like any <hyper>, <hypo>\n",
      "gpt2   like other <hyper>, <hypo>\n",
      "gpt2   <hyper> including <hypo>\n",
      "gpt2   <hypo> which is a class of <hyper>\n",
      "gpt2   <hypo> which is a kind of <hyper>\n",
      "gpt2   <hypo> which is a type of <hyper>\n",
      "gpt2   <hypo> (and|or) (any|some) other <hyper>\n",
      "gpt2   <hypo> and any other <hyper>\n",
      "gpt2   <hypo> and some other <hyper>\n",
      "gpt2   <hypo> or any other <hyper>\n",
      "gpt2   <hypo> is more specific than <hyper>\n",
      "gpt2   <hypo> is a type of <hyper>\n",
      "gpt2-medium   <hyper> is more general than <hypo>\n",
      "gpt2-medium   <hypo> which is a (example|class|kind|. . . ) of <hyper>\n",
      "gpt2-medium   <hypo> or some other <hyper>\n",
      "gpt2-medium   <hypo> which is called <hyper>\n",
      "gpt2-medium   <hypo> a special case of <hyper>\n",
      "gpt2-medium   <hypo> is an <hyper> that\n",
      "gpt2-medium   (Unlike|like) (most|all|any|other) <hyper>, <hypo>\n",
      "gpt2-medium   unlike most <hyper>, <hypo>\n",
      "gpt2-medium   unlike all <hyper>, <hypo>\n",
      "gpt2-medium   unlike any <hyper>, <hypo>\n",
      "gpt2-medium   unlike other <hyper>, <hypo>\n",
      "gpt2-medium   like most <hyper>, <hypo>\n",
      "gpt2-medium   <hypo> which is a example of <hyper>\n",
      "gpt2-medium   like all <hyper>, <hypo>\n",
      "gpt2-medium   like any <hyper>, <hypo>\n",
      "gpt2-medium   like other <hyper>, <hypo>\n",
      "gpt2-medium   <hyper> including <hypo>\n",
      "gpt2-medium   <hypo> which is a class of <hyper>\n",
      "gpt2-medium   <hypo> which is a kind of <hyper>\n",
      "gpt2-medium   <hypo> which is a type of <hyper>\n",
      "gpt2-medium   <hypo> (and|or) (any|some) other <hyper>\n",
      "gpt2-medium   <hypo> and any other <hyper>\n",
      "gpt2-medium   <hypo> and some other <hyper>\n",
      "gpt2-medium   <hypo> or any other <hyper>\n",
      "gpt2-medium   <hypo> is more specific than <hyper>\n",
      "gpt2-medium   <hypo> is a type of <hyper>\n",
      "gpt2-large   <hyper> is more general than <hypo>\n",
      "gpt2-large   <hypo> which is a (example|class|kind|. . . ) of <hyper>\n",
      "gpt2-large   <hypo> or some other <hyper>\n",
      "gpt2-large   <hypo> which is called <hyper>\n",
      "gpt2-large   <hypo> a special case of <hyper>\n",
      "gpt2-large   <hypo> is an <hyper> that\n",
      "gpt2-large   (Unlike|like) (most|all|any|other) <hyper>, <hypo>\n",
      "gpt2-large   unlike most <hyper>, <hypo>\n",
      "gpt2-large   unlike all <hyper>, <hypo>\n",
      "gpt2-large   unlike any <hyper>, <hypo>\n",
      "gpt2-large   unlike other <hyper>, <hypo>\n",
      "gpt2-large   like most <hyper>, <hypo>\n",
      "gpt2-large   <hypo> which is a example of <hyper>\n",
      "gpt2-large   like all <hyper>, <hypo>\n",
      "gpt2-large   like any <hyper>, <hypo>\n",
      "gpt2-large   like other <hyper>, <hypo>\n",
      "gpt2-large   <hyper> including <hypo>\n",
      "gpt2-large   <hypo> which is a class of <hyper>\n",
      "gpt2-large   <hypo> which is a kind of <hyper>\n",
      "gpt2-large   <hypo> which is a type of <hyper>\n",
      "gpt2-large   <hypo> (and|or) (any|some) other <hyper>\n",
      "gpt2-large   <hypo> and any other <hyper>\n",
      "gpt2-large   <hypo> and some other <hyper>\n",
      "gpt2-large   <hypo> or any other <hyper>\n",
      "gpt2-large   <hypo> is more specific than <hyper>\n",
      "gpt2-large   <hypo> is a type of <hyper>\n",
      "gpt2-xl   <hyper> is more general than <hypo>\n",
      "gpt2-xl   <hypo> which is a (example|class|kind|. . . ) of <hyper>\n",
      "gpt2-xl   <hypo> or some other <hyper>\n",
      "gpt2-xl   <hypo> which is called <hyper>\n",
      "gpt2-xl   <hypo> a special case of <hyper>\n",
      "gpt2-xl   <hypo> is an <hyper> that\n",
      "gpt2-xl   (Unlike|like) (most|all|any|other) <hyper>, <hypo>\n",
      "gpt2-xl   unlike most <hyper>, <hypo>\n",
      "gpt2-xl   unlike all <hyper>, <hypo>\n",
      "gpt2-xl   unlike any <hyper>, <hypo>\n",
      "gpt2-xl   unlike other <hyper>, <hypo>\n",
      "gpt2-xl   like most <hyper>, <hypo>\n",
      "gpt2-xl   <hypo> which is a example of <hyper>\n",
      "gpt2-xl   like all <hyper>, <hypo>\n",
      "gpt2-xl   like any <hyper>, <hypo>\n",
      "gpt2-xl   like other <hyper>, <hypo>\n",
      "gpt2-xl   <hyper> including <hypo>\n",
      "gpt2-xl   <hypo> which is a class of <hyper>\n",
      "gpt2-xl   <hypo> which is a kind of <hyper>\n",
      "gpt2-xl   <hypo> which is a type of <hyper>\n",
      "gpt2-xl   <hypo> (and|or) (any|some) other <hyper>\n",
      "gpt2-xl   <hypo> and any other <hyper>\n",
      "gpt2-xl   <hypo> and some other <hyper>\n",
      "gpt2-xl   <hypo> or any other <hyper>\n",
      "gpt2-xl   <hypo> is more specific than <hyper>\n",
      "gpt2-xl   <hypo> is a type of <hyper>\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "model_name = 'gpt2-xl'\n",
    "models = ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "prompt_eval_data = {}\n",
    "model_eval_data = {}\n",
    "for model_name in models:\n",
    "    for prompt_name in sorted(PROMPTS.keys()):\n",
    "        pattern = PROMPTS[prompt_name]\n",
    "        #hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n",
    "        #res = all_evaluations(hs_model)\n",
    "        print(model_name, ' ', pattern)\n",
    "        \n",
    "\n",
    "        #res['pattern'] = pattern\n",
    "        with codecs.open(f'output/{model_name}_{prompt_name}.json', 'r', 'utf-8') as file_descr:\n",
    "            res = json.load(file_descr)\n",
    "            table_data, mean = print_res_table(res, True)\n",
    "            if prompt_name not in prompt_eval_data:\n",
    "                prompt_eval_data[prompt_name] = []\n",
    "            prompt_eval_data[prompt_name].append(mean)\n",
    "\n",
    "            if model_name not in model_eval_data:\n",
    "                model_eval_data[model_name] = []\n",
    "            model_eval_data[model_name].append(mean)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hyper10', 0.6302596102485258],\n",
       " ['hyper13', 0.629544741055069],\n",
       " ['hyper9', 0.6276047237551061],\n",
       " ['hyper7', 0.6099226336488329],\n",
       " ['hyper21', 0.575173407484378],\n",
       " ['type', 0.5703122006602911],\n",
       " ['hyper6', 0.5578845267164985],\n",
       " ['hyper3', 0.5510195780118324],\n",
       " ['hyper17', 0.5473959547580849],\n",
       " ['hyper18', 0.5409707686995588],\n",
       " ['hyper8', 0.5400540871717074],\n",
       " ['hyper22', 0.5004303243717174],\n",
       " ['hyper19', 0.5002340698550253],\n",
       " ['hyper15', 0.49728501676016545],\n",
       " ['hyper5', 0.495150267138872],\n",
       " ['hyper20', 0.4894568597995863],\n",
       " ['hyper23', 0.4857014113642769],\n",
       " ['hyper2', 0.4856964757865996],\n",
       " ['hyper4', 0.4725073595139392],\n",
       " ['hyper16', 0.4696370338777007],\n",
       " ['hyper1', 0.4578795891479095],\n",
       " ['hyper14', 0.3886651891915355],\n",
       " ['spec', 0.37414205057870553],\n",
       " ['hyper12', 0.37338586050341005],\n",
       " ['gen', 0.36884028291939164],\n",
       " ['hyper11', 0.3370938709792304]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_eval_data = sorted([[name, np.mean(data)] for name, data in prompt_eval_data.items()], key=lambda x: -x[1])\n",
    "prompt_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gpt2-xl', 0.5216751845144054],\n",
       " ['gpt2-large', 0.5112516347244956],\n",
       " ['gpt2-medium', 0.4948383896321819],\n",
       " ['gpt2', 0.48396523635937094]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval_data = sorted([[name, np.mean(data)] for name, data in model_eval_data.items()], key=lambda x: -x[1])\n",
    "model_eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hyper10', 0.6302596102485258],\n",
       " ['hyper13', 0.629544741055069],\n",
       " ['hyper9', 0.6276047237551061],\n",
       " ['hyper7', 0.6099226336488329],\n",
       " ['hyper21', 0.575173407484378],\n",
       " ['type', 0.5703122006602911]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_prompts = prompt_eval_data[:6]\n",
    "top_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS = {\n",
    "    'gen': \"<hyper> is more general than <hypo>\",\n",
    "    'spec': \"<hypo> is more specific than <hyper>\",\n",
    "    'type': \"<hypo> is a type of <hyper>\",\n",
    "    'hyper1': \"<hypo> which is a (example|class|kind|. . . ) of <hyper>\",\n",
    "    'hyper2': \"<hypo> which is a example of <hyper>\",\n",
    "    'hyper3': \"<hypo> which is a class of <hyper>\",\n",
    "    'hyper4': \"<hypo> which is a kind of <hyper>\",\n",
    "    'hyper5': \"<hypo> which is a type of <hyper>\",\n",
    "    'hyper6': \"<hypo> (and|or) (any|some) other <hyper>\",\n",
    "    'hyper7': \"<hypo> and any other <hyper>\",\n",
    "    'hyper8': \"<hypo> and some other <hyper>\",\n",
    "    'hyper9': \"<hypo> or any other <hyper>\",\n",
    "    'hyper10': \"<hypo> or some other <hyper>\",\n",
    "    'hyper11': \"<hypo> which is called <hyper>\",\n",
    "    'hyper12': \"<hypo> a special case of <hyper>\",\n",
    "    'hyper13': \"<hypo> is an <hyper> that\",\n",
    "    'hyper14': \"(Unlike|like) (most|all|any|other) <hyper>, <hypo>\",\n",
    "    'hyper15': \"unlike most <hyper>, <hypo>\",\n",
    "    'hyper16': \"unlike all <hyper>, <hypo>\",\n",
    "    'hyper17': \"unlike any <hyper>, <hypo>\",\n",
    "    'hyper18': \"unlike other <hyper>, <hypo>\",\n",
    "    'hyper19': \"like most <hyper>, <hypo>\",\n",
    "    'hyper20': \"like all <hyper>, <hypo>\",\n",
    "    'hyper21': \"like any <hyper>, <hypo>\",\n",
    "    'hyper22': \"like other <hyper>, <hypo>\",\n",
    "    'hyper23': \"<hyper> including <hypo>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Envs/mtikhomi/lib/python3.6/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef408525c0a4537b11ad57df40af5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=200, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4d32a53c434c259c63df495961a2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=798156, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc57add35bf42928a700c04658f641e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=456356, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6bcca72e9c4b29b9162750269866f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=90, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "scorer = HFLMScorer(model_name, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hyper10', 'hyper13', 'hyper21', 'type']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_prompts = ['hyper10', 'hyper13', 'hyper21', 'type']\n",
    "top_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hyper10', 'hyper13']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_prompts = ['hyper10', 'hyper13']\n",
    "top_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [00:06<00:00,  8.23it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.28it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.34it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.32it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.23it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.22it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  8.06it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  8.05it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.96it/s]\n",
      "100%|██████████| 42/42 [00:05<00:00,  7.95it/s]\n",
      "100%|██████████| 68/68 [00:08<00:00,  8.48it/s]\n",
      "100%|██████████| 68/68 [00:08<00:00,  8.45it/s]\n",
      "100%|██████████| 452/452 [00:58<00:00,  7.70it/s]\n",
      "100%|██████████| 452/452 [00:59<00:00,  7.66it/s]\n",
      "100%|██████████| 87/87 [00:11<00:00,  7.84it/s]\n",
      "100%|██████████| 87/87 [00:11<00:00,  7.84it/s]\n",
      "100%|██████████| 421/421 [00:41<00:00, 10.07it/s]\n",
      "100%|██████████| 421/421 [00:41<00:00, 10.06it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.05it/s]\n",
      "100%|██████████| 53/53 [00:06<00:00,  8.04it/s]\n",
      "100%|██████████| 1644/1644 [04:13<00:00,  6.49it/s]\n",
      "100%|██████████| 1644/1644 [04:13<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-1.3B   <hypo> or some other <hyper>|<hypo> is an <hyper> that\n",
      "0,47 0,41 0,83 0,51 0,86 0,91 0,74 0,68 0,40 0,65\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'codecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-15841336d701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprompt_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'output/{mn}_{prompt_name}.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_descr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_descr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codecs' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(2, len(top_prompts) + 1):\n",
    "    prompt_names = [p for p in top_prompts[:i]]\n",
    "    patterns = [PROMPTS[p] for p in prompt_names]\n",
    "    hs_model = GPTHypernymySuiteModel(scorer, patterns, 'data')\n",
    "    res = all_evaluations(hs_model)\n",
    "    print(model_name, ' ', '|'.join(patterns))\n",
    "    print(print_res_table(res))\n",
    "\n",
    "    res['pattern'] = prompt_names\n",
    "    prompt_name = '|'.join(prompt_names)\n",
    "    mn = model_name.replace('/', '-')\n",
    "    with codecs.open(f'output/{mn}_{prompt_name}.json', 'w', 'utf-8') as file_descr:\n",
    "        json.dump(res, file_descr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'h', 'h']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-xl   <hypo> or some other <hyper>|<hypo> is an <hyper> that|like any <hyper>, <hypo>\n",
      "0,52 0,43 0,86 0,53 0,87 0,92 0,74 0,70 0,54 0,68\n",
      "gpt2-xl   <hypo> or some other <hyper>|<hypo> is an <hyper> that|like any <hyper>, <hypo>|<hypo> is a type of <hyper>\n",
      "0,51 0,41 0,87 0,51 0,86 0,93 0,73 0,69 0,56 0,67\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "model_name = 'gpt2-xl'\n",
    "for i in range(3, len(top_prompts) + 1):\n",
    "    prompt_names = [p for p in top_prompts[:i]]\n",
    "    patterns = [PROMPTS[p] for p in prompt_names]\n",
    "    #hs_model = GPTHypernymySuiteModel(scorer, patterns, 'data')\n",
    "    #res = all_evaluations(hs_model)\n",
    "    print(model_name, ' ', '|'.join(patterns))\n",
    "    #print(print_res_table(res))\n",
    "\n",
    "    #res['pattern'] = prompt_names\n",
    "    prompt_name = '|'.join(prompt_names)\n",
    "    #with codecs.open(f'output/{model_name}_{prompt_name}.json', 'w', 'utf-8') as file_descr:\n",
    "    #    json.dump(res, file_descr)\n",
    "    with codecs.open(f'output/{model_name}_{prompt_name}.json', 'r', 'utf-8') as file_descr:\n",
    "        res = json.load(file_descr)\n",
    "        table_data, mean = print_res_table(res, True)\n",
    "    print(table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "for prompt_name in PROMPTS:\n",
    "    pattern = PROMPTS[prompt_name]\n",
    "    hs_model = GPTHypernymySuiteModel(scorer, pattern, 'data')\n",
    "    res = all_evaluations(hs_model)\n",
    "    print(model_name, ' ', pattern)\n",
    "    print(print_res_table(res))\n",
    "\n",
    "    res['pattern'] = pattern\n",
    "    with codecs.open(f'output/{model_name}_{prompt_name}.json', 'w', 'utf-8') as file_descr:\n",
    "        json.dump(res, file_descr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mtikhomi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
